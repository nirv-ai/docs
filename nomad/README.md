# NIRVai Nomad docs

documentation for Nomad @ NIRVai

## RACEXP

- [NIRV DOCS project board](https://github.com/orgs/nirv-ai/projects/6/views/1?filterQuery=repo%3A%22nirv-ai%2Fdocs%22)
- [RACEXP docs](https://github.com/noahehall/theBookOfNoah/blob/master/0current/architectural%20thinking/0racexp.md)

## helpful links

- [error about cni plugin](https://discuss.hashicorp.com/t/failed-to-find-plugin-bridge-in-path/3095)
- [enabling bind mounts](https://developer.hashicorp.com/nomad/docs/drivers/docker#enabled-1)
- [vault container requires ipc_lock](https://developer.hashicorp.com/nomad/docs/drivers/docker#allow_caps)

## development validation workflow

### REQUIREMENTS

- [you have nirvai/scripts in your path](../scripts/README.md)
- [you have nomad development configs](../configs/README.md)
  - copypasta the configs: `core/apps/nomad app/dev/configs-go-here`
- you have a [local docker registry setup](../docker/README.md)
- you've setup [TLS with cloudflare cfssl](./tls.md)

- your directory structure should look like this before continuing

```sh
./you-are-here
├── configs # source of development.*.nomad configs
├── core # your monorepo
    ├── apps
        ├── nirvai-core-nomad # nomad application
            ├── dev # nomad development directory
                ├── tls # where we will store keys generated by cfssl
                    └── server.pem
                    ├── cfssl.json
                    ├── cli-key.pem
                    ├── cli.csr
                    ├── cli.pem
                    ├── client-key.pem
                    ├── client.csr
                    ├── client.pem
                    ├── nomad-ca-key.pem
                    ├── nomad-ca.csr
                    ├── nomad-ca.pem
                    ├── server-key.pem
                    ├── server.csr
                ├── development.client.nomad # nomad client agent config
                ├── development.server.nomad # nomad server agent config
                ├── development.dev_core.nomad # nomad jobspec

```

### INTERFACE

```sh
NOMAD_ADDR_SUBD=${ENV:-dev}
NOMAD_ADDR_HOST=${NOMAD_ADDR_HOST:-nirv.ai}
NOMAD_SERVER_PORT="${NOMAD_SERVER_PORT:-4646}"
NOMAD_ADDR="${NOMAD_ADDR:-https://${NOMAD_ADDR_SUBD}.${NOMAD_ADDR_HOST}:${NOMAD_SERVER_PORT}}"
NOMAD_CACERT="${NOMAD_CACERT:-./tls/nomad-ca.pem}"
NOMAD_CLIENT_CERT="${NOMAD_CLIENT_CERT:-./tls/cli.pem}"
NOMAD_CLIENT_KEY="${NOMAD_CLIENT_KEY:-./tls/cli-key.pem}"

```

### setup env for development validation in Nomad

- transitioning from: active development in docker compose
  - e.g. where apps are developed and tests are executed
- transitioning to: development validation in nomad
  - e.g. when e2e tests are executed and _validated_ and PRs are ready for review

```sh
# in all examples: `$CORE` is the root of your monorepo

########### cd $CORE
# start a docker registry
script.registry.sh run


# skip this step if:
## ^ your local images are up to date
## ^ you havent made any changes to development environment variables

## recreate .env.development.compose.{json,yaml} and symlink to nomad/dev
script.refresh.compose.sh
ln -s ./.env.development.compose.* ./apps/nirvai-core-nomad/dev/


# skip this step if:
## ^ your local registry has the latest images

## tag-and-push all images backing running containers to the registry
## if your containers arent running use `tag IMAGE_NAME`
script.registry.sh tag_running

## stop all development containers (nomad uses the images in the registry)
docker compose down

```

### start nomad server and client agents

```sh
########### cd ./apps/nirvai-core-nomad/dev
# start server agent in bg
script.nmd.sh start s -config=development.server.nomad

# start client agent in bg
script.nmd.sh start c -config=development.client.nomad

# check the team status
script.nmd.sh get status team

```

### deploy nomad jobspec

```sh
# if ./development.dev_core.nomad doesnt exist
# create it and get the index number for stdout
script.nmd.sh get plan dev_core

# deploy job dev_core
script.nmd.sh run dev_core indexNumber
script.nmd.sh dockerlogs # [optional] see logs of all running containers

# on error run
script.nmd.sh get status job jobName # includes all allocationIds
script.nmd.sh get status loc allocationId # in event of deployment failure
script.nmd.sh get status node # see client nodes and there ids
script.nmd.sh dockerlogs # following docker logs of all running containers
nomad alloc exec -i -t -task sidekiq fa2b2ed6 /bin/bash # todo,
nomad alloc exec -i -t -task puma fa2b2ed6 /bin/bash -c "bundle exec rails c" #todo
nomad job history -p job_name # todo

# cleanup
# rm the job
script.nmd.sh rm dev_core

# kill the team @see https://github.com/noahehall/theBookOfNoah/blob/master/linux/bash_cli_fns/000util.sh
kill_service_by_name nomad

# reset nomad to a green state if you dont plan on using it later
nomad system gc
```

### run e2e tests against deployed services

- TODO: still havent setup playwright

## scripts

- [scripting architecture & guidance](../scripts/README.md)

### script.nmd.sh

- [source code](https://github.com/nirv-ai/scripts/blob/develop/nomad)
- [access nomad UI for nirvai-core @ https://mad.nirv.ai:4646](https://mad.nirv.ai:4646/ui/jobs)

```sh
###################### USAGE
## prefix all cmds with script.nmd.sh poop poop poop
## poop being one of the below

# check on the server
get status team
get status all
dockerlogs # following logs of all running containers

# creating stuff
create gossipkey
create job myJobName
get plan myJobName # provides indexNumber

# running stuff
run myJobName indexNumber

# restarting stuff
restart loc allocationId taskName # todo: https://developer.hashicorp.com/nomad/docs/commands/alloc/restart

# execing stuff
exec loc  allocationId cmd .... @ todo https://developer.hashicorp.com/nomad/docs/commands/alloc/exec

# checking on running/failing stuff
get status node # see client nodes and there ids
get status node nodeId # provding a clients nodeId is super helpful; also provides allocationId
get status loc allocationId # super helpful for checking on failed jobs, provides deployment id
get status dep deploymentId # super helpful
get logs jobName deploymentId

# stopping stuff
stop job myJobName
rm myJobName # this purges the job
gc # purge system

```
